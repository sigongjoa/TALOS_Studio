# Line Detection Visualization and CI/CD Deployment Plan

## 1. Objective
To create an HTML page that visualizes the output of various line and curve detection steps for 5 input images, arranged in a 5x5 grid. The entire process, including image processing and HTML generation, will be automated and deployed via CI/CD (GitHub Actions).

## 2. Input Images
- `/mnt/d/progress/TALOS_Studio/ref/9007332256_494354581_52d22d527725b5f1e148fbfc4cd38644.jpg`
- `/mnt/d/progress/TALOS_Studio/ref/9007332256_494354581_55bd5db1c16c0c5d0878ec89a08eb0fa.jpg`
- `/mnt/d/progress/TALOS_Studio/ref/9007332256_494354581_688778cbd035d6d37a3ffc3b408f1b40.jpg`
- `/mnt/d/progress/TALOS_Studio/ref/9007332256_494354581_b4431b5b6edd1e0871d023193f40ffff.jpg`
- `/mnt/d/progress/TALOS_Studio/ref/9007332256_494354581_fe61fe9b128055cdb3949527920d057d.jpg`

## 3. Output Page Structure (5x5 Grid)
For each input image, the following 5 processed outputs will be displayed:

| Column 1           | Column 2                 | Column 3                 | Column 4                 | Column 5                 |
| :----------------- | :----------------------- | :----------------------- | :----------------------- | :----------------------- |
| **Original Image** | **Manga Line Extraction**| **Inverted B/W Lines**   | **Line Detection (SOLD2)**| **Curve Detection (Canny)**|

## 4. Detailed Plan

### Phase 1: Backend - Image Processing Script (`generate_visualizations.py`)

This script will orchestrate all image processing steps for multiple input images.

1.  **Input:** A list of `input_image_paths`, `output_base_dir`.
2.  **Output Structure:** For each input image, a unique subdirectory will be created within `output_base_dir` (e.g., `output_base_dir/image_01/`, `output_base_dir/image_02/`, etc.). Within each subdirectory, the 5 processed images will be saved.
3.  **Core Processing Loop (for each input image):**
    *   **Panel 1 (Original Image):** Copy the `input_image_path` to `output_sub_dir/original.png`.
    *   **Panel 2 (Manga Line Extraction):**
        *   Call `run_manga_line_extraction.run_manga_line_extraction_inference(input_image_path, output_sub_dir/manga_lines.png)`.
    *   **Panel 3 (Black/White & Inverted Lines):**
        *   **Function `process_for_panel3(manga_line_image_path, output_path)`:**
            *   Load `manga_line_image_path` (grayscale).
            *   Convert non-white pixels to black (0), and white pixels to white (255).
            *   Invert colors: `255 - pixel_value`.
            *   Save as `output_path`.
    *   **Panel 4 (Line Detection on Manga Lines using SOLD2):**
        *   Call `run_sold2.run_sold2_inference(output_sub_dir/manga_lines.png, output_sub_dir/line_detection.png)`. (Requires modification of `run_sold2.py` to expose this function).
    *   **Panel 5 (Curve Detection on Manga Lines using Canny):**
        *   **Function `process_for_panel5(manga_line_image_path, output_path)`:**
            *   Load `manga_line_image_path` (grayscale).
            *   Apply Canny edge detection (e.g., `cv2.Canny`).
            *   Save as `output_path`.

#### Definition of Done (DoD) for Phase 1:
- `generate_visualizations.py` script is created and runnable.
- It successfully processes all 5 input images.
- For each input image, 5 output images are correctly generated and saved in their respective subdirectories.
- All processing steps (Manga Line Extraction, B/W & Invert, SOLD2 Line Detection, Canny Curve Detection) execute without errors.
- `index.html` is dynamically generated with correct image paths.

#### Versioning for Phase 1:
- **Python:** 3.8 (due to LETR's environment requirements)
- **OpenCV:** Version compatible with `cv2` in `letr` environment (e.g., `opencv-python`)
- **PyTorch:** 1.9.0
- **Torchvision:** 0.10.0
- **Other Python packages:** `numpy`, `Pillow`

### Phase 2: Frontend - HTML File (`index.html`)

This HTML file will be dynamically generated by the Python script.

1.  **Structure:** Create a responsive HTML table or CSS grid layout (5 rows x 5 columns).
    *   The first row will contain column headers.
    *   Each subsequent row will correspond to one input image, displaying its original and 4 processed outputs.
2.  **Image Paths:** Dynamically embed the relative paths to the generated image files in `<img>` tags.
3.  **Styling:** Basic CSS for layout, image sizing, and readability.

#### Definition of Done (DoD) for Phase 2:
- `index.html` is generated with a 5x5 grid layout.
- All 25 image slots are correctly populated with the corresponding processed images.
- Column headers are present and correct.
- Basic styling ensures readability and proper display of images.
- The page is viewable in a web browser without layout issues.

#### Versioning for Phase 2:
- **HTML:** HTML5
- **CSS:** CSS3

### Phase 3: CI/CD Integration (GitHub Actions)

This phase involves setting up automated deployment of the generated visualization page.

1.  **Workflow File:** Create a GitHub Actions workflow file (`.github/workflows/deploy.yml`).
2.  **Trigger:** The workflow will be triggered on pushes to the `main` or `master` branch.
3.  **Steps:**
    *   **Checkout Code:** Get the repository code.
    *   **Set up Python Environment:** Configure Python (e.g., `python=3.8`).
    *   **Install Dependencies:**
        *   Install `conda`.
        *   Create and activate the `letr` conda environment (as per previous steps).
        *   Install all necessary `pip` packages for `MangaLineExtraction_PyTorch`, `SOLD2`, `LETR`, `ScaleLSD`, and OpenCV.
        *   Download `erika.pth` and `LETR` checkpoint.
    *   **Run `generate_visualizations.py`:** Execute the main Python script to perform image processing and generate `index.html` and all output images.
    *   **Deploy to GitHub Pages:** Use an action (e.g., `peaceiris/actions-gh-pages`) to deploy the `output_base_dir` contents to GitHub Pages.

#### Definition of Done (DoD) for Phase 3:
- `.github/workflows/deploy.yml` file is created.
- The workflow is triggered on push to `main` (or `master`).
- The workflow successfully sets up the environment, installs dependencies, runs `generate_visualizations.py`.
- The workflow successfully deploys the generated `index.html` and image assets to GitHub Pages.
- The deployed page is publicly accessible and displays correctly.

#### Versioning for Phase 3:
- **GitHub Actions:** Latest available runners (e.g., `ubuntu-latest`)
- **Python:** 3.8
- **Conda:** Latest stable version

## 5. Pre-computation/Pre-analysis (before full implementation)

1.  **Modify `run_sold2.py`:** Extract the core inference logic into a function `run_sold2_inference(input_image_path, output_image_path)`. Update its `if __name__ == "__main__":` block to call this function with default paths.
2.  **Ensure OpenCV:** Confirm OpenCV is installed in the environment where `generate_visualizations.py` will run (it should be, as MangaLineExtraction uses `cv2`).

## 6. Next Steps

Proceed with modifying `run_sold2.py` to expose its inference logic as a callable function.
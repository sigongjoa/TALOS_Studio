import requests
import json
import os
import re

"""
llm_connector.py

This module is responsible for handling the communication with the Large Language Model (LLM) API.
It constructs API requests, sends prompts generated by `prompt_builder.py`, and parses the
LLM's JSON responses. It also includes error handling for API calls and JSON parsing.

"""

def call_llm_api(prompt: str) -> dict:
    """
    Sends the constructed prompt to the LLM API and retrieves its JSON response.
    Handles API authentication and basic error checking.

    Args:
        prompt (str): The detailed prompt string generated by `build_llm_prompt`.

    Returns:
        dict: The parsed JSON response from the LLM, containing structured motion parameters.

    Raises:
        requests.exceptions.RequestException: For network-related errors or invalid API responses.
        ValueError: If the LLM response is not valid JSON or does not conform to expected structure.
    """
    ollama_url = os.environ.get("OLLAMA_API_URL", "http://localhost:11434/api/chat")
    ollama_model = os.environ.get("OLLAMA_MODEL_NAME", "llama2") # Default to llama2, user can change

    payload = {
        "model": ollama_model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "stream": False # We want a single response, not a stream
    }

    try:
        response = requests.post(ollama_url, json=payload)
        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
        llm_response = response.json()

        if "message" in llm_response and "content" in llm_response["message"]:
            content_str = llm_response["message"]["content"]
            
            # Use regex to find the JSON block, robust to surrounding text
            json_match = re.search(r'```json\n(.*?)```', content_str, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
            else:
                # If no markdown block, try to find a bare JSON object
                json_match = re.search(r'({.*})', content_str, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1).strip()
                else:
                    raise ValueError("LLM response content does not contain a valid JSON block.")

            try:
                parsed_content = json.loads(json_str)
                return parsed_content
            except json.JSONDecodeError:
                # If it's not a full JSON object, try to see if it's a JSON array of parameters
                try:
                    parsed_array = json.loads(json_str)
                    if isinstance(parsed_array, list):
                        # Wrap the array in a dummy JSON object
                        return {
                            "action": "generated_motion",
                            "duration": 1.0, # Default duration
                            "parameters": parsed_array
                        }
                except json.JSONDecodeError:
                    pass # Fall through to the original error
                raise ValueError(f'LLM returned non-JSON content within its message: {json_str}')
        else:
            raise ValueError("Unexpected LLM response format: 'message' or 'content' not found.")

    except requests.exceptions.RequestException as e:
        print(f"LLM API request failed: {e}")
        raise
    except json.JSONDecodeError:
        print("LLM API returned non-JSON response.")
        raise ValueError("Invalid JSON response from LLM API.")

if __name__ == '__main__':
    # Set a dummy API key for testing purposes (not used by Ollama directly, but kept for consistency if needed elsewhere)
    os.environ["LLM_API_KEY"] = "dummy_api_key_for_testing"
    os.environ["OLLAMA_API_URL"] = "http://localhost:11434/api/chat" # Ensure this is set for local testing
    os.environ["OLLAMA_MODEL_NAME"] = "llama2" # Or any other model you have pulled

    test_prompt = """
    오직 JSON 객체만 출력해야 해. 어떤 추가적인 설명이나 대화도 없이, 오직 JSON 객체만 출력해야 해.
    너는 OpenSim 시뮬레이션을 위한 동작 데이터를 생성하는 AI 에이전트야.
    사용자의 요청을 분석해서, OpenSim이 이해할 수 있는 정확한 JSON 형식의 데이터를 출력해야 해.
    JSON은 다음 필드를 포함해야 해:
    - 'action': (문자열) 동작에 대한 설명적인 이름.
    - 'duration': (실수) 동작의 예상 지속 시간(초).
    - 'parameters': (객체 배열) 각 객체는 관절 또는 신체 부위와 목표 상태를 나타내.
        - 'joint': (문자열) 관절의 이름 (예: 'pelvis_tilt', 'hip_flexion_l', 'knee_angle_r').
        - 'target_angle_x': (실수, 선택 사항) X축 회전의 목표 각도(도).
        - 'target_angle_y': (실수, 선택 사항) Y축 회전의 목표 각도(도).
        - 'target_angle_z': (실수, 선택 사항) Z축 회전의 목표 각도(도).
        - 'muscle_activation': (실수, 선택 사항) 근육 활성화 수준 (0.0 ~ 1.0).
        - 'target_position_x': (실수, 선택 사항) X축의 목표 위치(미터).
        - 'target_position_y': (실수, 선택 사항) Y축의 목표 위치(미터).
        - 'target_position_z': (실수, 선택 사항) Z축의 목표 위치(미터).
    동작과 관련된 매개변수만 포함하고, 지정되지 않은 매개변수는 생략해.
    관절 이름은 OpenSim 모델의 실제 관절 이름을 사용해야 해. 예를 들어, '왼쪽 엉덩이'는 'hip_flexion_l'이 될 수 있어.

    예시 JSON 형식:
    ```json
    {{
        "action": "wave_hand",
        "duration": 2.0,
        "parameters": [
            {{
                "joint": "elbow_L",
                "target_angle_x": 90.0
            }},
            {{
                "joint": "shoulder_R",
                "muscle_activation": 0.8
            }}
        ]
    }}
    ```

    사용자 요청: '{user_input_text}'
    오직 JSON 객체만 출력해야 해. 어떤 추가적인 설명이나 대화도 없이, 오직 JSON 객체만 출력해야 해.
    """
    try:
        print(f"Calling LLM API with prompt (truncated): {test_prompt[:100]}...")
        response_data = call_llm_api(test_prompt)
        print(json.dumps(response_data, indent=2))
    except (requests.exceptions.RequestException, ValueError) as e:
        print(f"Error during LLM API call test: {e}")

